{"cells":[{"cell_type":"markdown","metadata":{"id":"LYa6BF0IA5oV"},"source":["# 1.Installationen\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":12815,"status":"ok","timestamp":1763630740016,"user":{"displayName":"Christoph Nachname","userId":"04640672557127032436"},"user_tz":-60},"id":"4ChdipO5KQ50","outputId":"3acab00f-be9a-44fa-e838-434d4bbf190c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n","Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.3)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n","Collecting stable-baselines3[extra]\n","  Downloading stable_baselines3-2.7.0-py3-none-any.whl.metadata (4.8 kB)\n","Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.0.2)\n","Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.8.0+cu126)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (3.1.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.2.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (3.10.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (4.12.0.88)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.6.1)\n","Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.19.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (5.9.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (4.67.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (13.9.4)\n","Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (0.11.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (11.3.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n","Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.44.0)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (8.5.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.76.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.10)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.2.0)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.17.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.20.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.4.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.9)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->stable-baselines3[extra]) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->stable-baselines3[extra]) (2.19.2)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3[extra]) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.3)\n","Downloading stable_baselines3-2.7.0-py3-none-any.whl (187 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: stable-baselines3\n","Successfully installed stable-baselines3-2.7.0\n"]}],"source":["!pip install stable-baselines3[extra] gymnasium wandb plotly"]},{"cell_type":"markdown","metadata":{"id":"49-_j-LXBSgh"},"source":["#2.Globale Importe"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":27438,"status":"ok","timestamp":1763630767460,"user":{"displayName":"Christoph Nachname","userId":"04640672557127032436"},"user_tz":-60},"id":"usf9fHh4E1FC","outputId":"6c4bd643-838b-4d8f-fec9-9f4491508d89"},"outputs":[{"output_type":"stream","name":"stderr","text":["Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n","Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n","See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n","/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n","  return datetime.utcnow().replace(tzinfo=utc)\n"]}],"source":["## 2. Globale Importe\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import gymnasium as gym\n","from gymnasium import spaces\n","import wandb\n","from wandb.integration.sb3 import WandbCallback as OfficialWandbCallback # Wichtig!\n","from stable_baselines3 import PPO\n","from stable_baselines3.common.env_util import make_vec_env\n","from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n","from stable_baselines3.common.callbacks import CallbackList, BaseCallback, EvalCallback\n","from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n","import torch as th\n","from torch import nn\n","import torch.optim as optim\n","from google.colab import drive\n","from google.colab import userdata\n","import plotly.express as px\n","import uuid\n","import traceback"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"elapsed":50877,"status":"error","timestamp":1763630818342,"user":{"displayName":"Christoph Nachname","userId":"04640672557127032436"},"user_tz":-60},"id":"sm9NjcNZpfTg","outputId":"c215a32e-f9f4-4898-ddda-1fbb28588725"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounting Google Drive...\n"]},{"output_type":"error","ename":"MessageError","evalue":"Error: credential propagation was unsuccessful","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1224091572.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# --- 3.1 Google Drive & W&B Login ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mounting Google Drive...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# W&B Login\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["## 3. Globale Konfiguration (Setup, Pfade, Hyperparameter)\n","\n","# --- 3.1 Google Drive & W&B Login ---\n","print(\"Mounting Google Drive...\")\n","drive.mount('/content/drive')\n","\n","# W&B Login\n","if \"WANDB_API_KEY\" not in os.environ:\n","    try:\n","        os.environ[\"WANDB_API_KEY\"] = userdata.get('WANDB_API_KEY')\n","    except Exception:\n","        print(\"W&B API Key nicht in Colab Secrets gefunden. Bitte manuell eingeben.\")\n","\n","wandb.login()\n","\n","# --- 3.2 Pfad-Definitionen ---\n","print(\"Definiere Projektpfade...\")\n","BASE_PROJECT_PATH = '/content/drive/MyDrive/01_Data/projects/PPO_portfolio_optimization/'\n","PROCESSED_DATA_PATH = os.path.join(BASE_PROJECT_PATH, 'processed_data')\n","MODEL_SAVE_PATH = os.path.join(BASE_PROJECT_PATH, 'models')\n","LOG_BASE_PATH = os.path.join(BASE_PROJECT_PATH, 'data/wandb_logs') # Für Tensorboard\n","os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n","os.makedirs(PROCESSED_DATA_PATH, exist_ok=True)\n","os.makedirs(LOG_BASE_PATH, exist_ok=True)\n","\n","CLEANED_FEATURES_FILE = os.path.join(PROCESSED_DATA_PATH, 'features_cleaned.csv')\n","TRAIN_TEST_SPLIT_DATE = '2020-01-01'\n","\n","# --- 3.3 W&B Projekt-Definition ---\n","WANDB_ENTITY = \"cb-ml\" # Bitte anpassen\n","WANDB_PROJECT_NAME = \"PPO_Portfolio_SP500\" # Bitte anpassen\n","\n","# --- 3.4 Hardware-Setup ---\n","DEVICE = \"cuda\" if th.cuda.is_available() else \"cpu\"\n","NUM_CPU_CORES = os.cpu_count() or 1\n","DEFAULT_SEED = 42\n","print(f\"Hardware: {DEVICE}, Parallele CPU-Kerne: {NUM_CPU_CORES}\")\n","\n","# --- 3.5 Lade Daten ---\n","print(f\"Lade Daten aus {CLEANED_FEATURES_FILE}...\")\n","try:\n","    data_df = pd.read_csv(CLEANED_FEATURES_FILE, index_col=0, header=[0, 1], parse_dates=True)\n","\n","    # Dynamische Asset- und Feature-Zählung\n","    NUM_ASSETS = len(data_df.columns.get_level_values(1).unique())\n","    NUM_FEATURES = len(data_df.columns.get_level_values(0).unique())\n","\n","    # Split train/eval\n","    train_data = data_df.loc[data_df.index < TRAIN_TEST_SPLIT_DATE].copy()\n","    eval_data = data_df.loc[data_df.index >= TRAIN_TEST_SPLIT_DATE].copy()\n","\n","    N_ASSETS_PLUS_CASH = NUM_ASSETS + 1\n","\n","    print(f\"Daten geladen. {NUM_ASSETS} Assets, {NUM_FEATURES} Features.\")\n","    print(f\"Trainings-Daten: {len(train_data)} Zeilen, Eval-Daten: {len(eval_data)} Zeilen.\")\n","\n","except FileNotFoundError:\n","    print(f\"FATALER FEHLER: Konnte {CLEANED_FEATURES_FILE} nicht finden. Bitte Notebook 02 ausführen!\")\n","    raise\n","except Exception as e:\n","    print(f\"Fehler beim Laden oder Splitten der Daten: {e}\")\n","    raise\n","\n","# --- 3.6 Environment-Konfiguration (ENV_KWARGS) ---\n","ENV_KWARGS = {\n","    \"initial_capital\": 10000.0,\n","    \"max_drawdown\": 0.5,       # 50% max. Verlust\n","    \"transaction_cost_pct\": 0.001, # 0.1% Kosten\n","    \"lookback_window\": 10,\n","    \"bankrupt_threshold\": 1.0  # Unter 1.0 = Bankrott\n","}\n","\n","# ##################################################################\n","# ### Alle Trainings-Parameter hier zusammengefasst ###\n","# ##################################################################\n","\n","# --- 3.7 Trainings-Parameter ---\n","TOTAL_TIMESTEPS = 1_000_000 # 1 Million Schritte\n","EVAL_FREQ = 20_000        # Alle 20k Schritte evaluieren\n","N_EVAL_EPISODES = 1       # Nur 1 volle Eval-Episode\n","LOG_FREQ = 1000           # (Wird von OfficialWandbCallback nicht direkt genutzt)\n","\n","# --- 3.8 Architektur- & PPO-Hyperparameter ---\n","NN_ACTIVATIONS = {\n","    \"ReLU\": nn.ReLU,\n","    \"Tanh\": nn.Tanh,\n","    \"Sigmoid\": nn.Sigmoid,\n","}\n","\n","OPTIMIZERS = {\n","    \"Adam\": th.optim.Adam,\n","    \"RMSprop\": th.optim.RMSprop,\n","    \"SGD\": th.optim.SGD,\n","}\n","\n","# Dies sind die Parameter aus Ihrer alten Zelle 12\n","PPO_PARAMS_CONFIG = {\n","    \"features_dim\": 128,      # Output des CustomExtractors\n","    \"policy_width\": 256,      # Breite der Actor-Netzwerke\n","    \"policy_depth\": 3,        # Tiefe der Actor-Netzwerke\n","    \"value_width\": 256,       # Breite der Critic-Netzwerke\n","    \"value_depth\": 3,         # Tiefe der Critic-Netzwerke\n","    \"activation_fn\": \"ReLU\",  # Aktivierungsfunktion\n","    \"optimizer\": \"Adam\",      # Optimizer\n","    \"learning_rate\": 0.0001,  # 1e-4\n","    \"n_steps\": 2048,          # Schritte pro PPO-Update (Rollout-Buffer-Größe)\n","    \"batch_size\": 64,         # Mini-Batch-Größe\n","    \"n_epochs\": 10,           # Epochen pro Update\n","    \"gamma\": 0.99,            # Diskontierungsfaktor\n","    \"gae_lambda\": 0.95,       # GAE-Lambda\n","    \"clip_range\": 0.2,        # PPO-Clipping\n","    \"ent_coef\": 0.005,        # Entropie-Koeffizient\n","    \"vf_coef\": 0.5,           # Value-Function-Koeffizient\n","    \"max_grad_norm\": 0.5      # Gradient-Clipping\n","}\n","# ##################################################################\n","\n","print(\"Globale Konfiguration (Zelle 3) abgeschlossen.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GvvxJD8DE1FE","executionInfo":{"status":"aborted","timestamp":1763630818339,"user_tz":-60,"elapsed":91419,"user":{"displayName":"Christoph Nachname","userId":"04640672557127032436"}}},"outputs":[],"source":["## 4. Custom Portfolio Environment (Gymnasium)\n","\n","class PortfolioEnv(gym.Env):\n","    \"\"\"\n","    Custom Environment für Portfolio-Optimierung (basierend auf Stable_Baseline_V4).\n","    - Observation Space: Dict (Features [LSTM Input], Portfolio Weights [MLP Input])\n","    - Action Space: Box (Logits für eine Dirichlet-Verteilung)\n","    - Reward: Log-Return des Portfolios.\n","    - Termination: Max Drawdown oder Bankrott.\n","    \"\"\"\n","    metadata = {'render_modes': ['human'], 'render_fps': 30}\n","\n","    def __init__(self, data_df: pd.DataFrame, **kwargs):\n","        super(PortfolioEnv, self).__init__()\n","\n","        self.data = data_df\n","        # Price Ratios sind exp(Log Returns).\n","        self.price_ratios = np.exp(data_df.loc[:, 'LogReturns'].values)\n","\n","        # Konfiguration (wird von Zelle 3 übergeben)\n","        self.initial_capital = kwargs.get(\"initial_capital\", 10000.0)\n","        self.max_drawdown = kwargs.get(\"max_drawdown\", 0.5)\n","        self.transaction_cost_pct = kwargs.get(\"transaction_cost_pct\", 0.001)\n","        self.lookback_window = kwargs.get(\"lookback_window\", 10)\n","        self.bankrupt_threshold = kwargs.get(\"bankrupt_threshold\", 1.0)\n","\n","        # Dimensionen (aus Zelle 3 geladen)\n","        self.num_assets = NUM_ASSETS\n","        self.num_features = NUM_FEATURES\n","        self.N_ASSETS_PLUS_CASH = N_ASSETS_PLUS_CASH # Cash ist Index 0\n","\n","        # State\n","        self.current_step = self.lookback_window # Start nach Lookback-Window\n","        self.max_steps = len(data_df) - 1\n","        self.current_portfolio_value = self.initial_capital\n","        self.peak_portfolio_value = self.initial_capital\n","\n","        # Initialisierung der Gewichte (Cash = 100%)\n","        self.current_weights = np.zeros(self.N_ASSETS_PLUS_CASH, dtype=np.float32)\n","        self.current_weights[0] = 1.0\n","\n","        # Action Space (Logits, die zu Alphas werden)\n","        self.action_space = spaces.Box(low=-100.0, high=100.0, shape=(self.N_ASSETS_PLUS_CASH,), dtype=np.float32)\n","\n","\n","        # Observation Space (Input für Actor-Critic Netzwerke)\n","        self.observation_space = spaces.Dict({\n","            # Features (L, N_Assets, N_Features) -> LSTM-Input\n","            'features': spaces.Box(low=-np.inf, high=np.inf,\n","                                   shape=(self.lookback_window, self.num_assets, self.num_features),\n","                                   dtype=np.float32),\n","            # Portfolio Weights (N_Assets + Cash) -> MLP-Input\n","            'portfolio_weights': spaces.Box(low=0.0, high=1.0,\n","                                            shape=(self.N_ASSETS_PLUS_CASH,),\n","                                            dtype=np.float32)\n","        })\n","\n","        # --- Daten-Validierung (Price Ratios) ---\n","        max_ratio = np.max(self.price_ratios)\n","        min_ratio = np.min(self.price_ratios)\n","        if np.isinf(max_ratio) or np.isinf(min_ratio) or np.isnan(max_ratio) or np.isnan(min_ratio):\n","             print(\"\\n\" + \"=\"*70)\n","             print(f\"FATALER FEHLER: 'inf' oder 'NaN' in price_ratios gefunden.\")\n","             print(f\"Max: {max_ratio}, Min: {min_ratio}\")\n","             print(\"Bitte Notebook 02 ausführen und LogReturns clippen!\")\n","             print(\"=\"*70 + \"\\n\")\n","             raise ValueError(\"inf/nan in price_ratios\")\n","\n","        print(\"--- Daten-Validierung (Price Ratios) ---\")\n","        print(f\"Max Price Ratio: {max_ratio}\")\n","        print(f\"Min Price Ratio: {min_ratio}\")\n","        print(f\"Mean Price Ratio: {np.mean(self.price_ratios)}\")\n","        print(f\"NaNs in Price Ratios: {np.isnan(self.price_ratios).sum()}\")\n","        print(\"------------------------------------------\")\n","\n","\n","    def _get_obs(self):\n","        start = self.current_step - self.lookback_window\n","        end = self.current_step\n","        feature_history_df = self.data.iloc[start:end]\n","\n","        # (L, N_Features, N_Assets) -> (L, N_Assets, N_Features)\n","        feature_history = feature_history_df.values.reshape(\n","            self.lookback_window, self.num_features, self.num_assets\n","        ).swapaxes(1, 2)\n","\n","        return {\n","            'features': feature_history.astype(np.float32),\n","            'portfolio_weights': self.current_weights.astype(np.float32)\n","        }\n","\n","\n","    def step(self, action: np.ndarray):\n","\n","        # ##################################################################\n","        # ### HIER IST DER FIX FÜR DEN SLOWDOWN ###\n","        # ##################################################################\n","        #\n","        # ALT (Instabil und langsam):\n","        # alpha = np.log(1.0 + np.exp(action)) + 1e-6\n","        #\n","        # NEU (Numerisch stabil und schnell):\n","        # np.logaddexp(0, action) ist äquivalent zu log(exp(0) + exp(action))\n","        # = log(1 + exp(action))\n","        alpha = np.logaddexp(0, action) + 1e-6\n","        # ##################################################################\n","\n","        # 1. Aktion zu Portfolio-Gewichten (Dirichlet)\n","        new_weights = alpha / np.sum(alpha)\n","\n","\n","        # 2. Portfolio-Wertentwicklung berechnen\n","        asset_ratios_today = self.price_ratios[self.current_step]\n","        price_ratios_with_cash = np.insert(asset_ratios_today, 0, 1.0)\n","        portfolio_return_before_costs = np.dot(new_weights, price_ratios_with_cash)\n","\n","        # 3. Transaktionskosten berechnen\n","        turnover = np.sum(np.abs(new_weights[1:] - self.current_weights[1:]))\n","        cost = turnover * self.transaction_cost_pct\n","\n","        # 4. Finaler Return und neuer Portfolio-Wert\n","        portfolio_return_after_costs = portfolio_return_before_costs - cost\n","        portfolio_return_after_costs = max(portfolio_return_after_costs, 0)\n","\n","        prev_portfolio_value = self.current_portfolio_value\n","        self.current_portfolio_value *= portfolio_return_after_costs\n","\n","        # 5. Reward (Log-Return)\n","        reward = np.log(max(portfolio_return_after_costs, 1e-10))\n","\n","        # 6. Metriken und State-Update\n","        self.current_step += 1\n","        self.current_weights = new_weights\n","\n","        # Drawdown-Berechnung\n","        self.peak_portfolio_value = max(self.peak_portfolio_value, self.current_portfolio_value)\n","        current_drawdown = (self.peak_portfolio_value - self.current_portfolio_value) / self.peak_portfolio_value\n","\n","        # 7. Termination und Truncation\n","        terminated = False\n","        truncated = False\n","\n","        if self.current_step >= self.max_steps:\n","            terminated = True\n","        if current_drawdown > self.max_drawdown:\n","            terminated = True\n","            reward = -10.0 # Harte Strafe\n","        if self.current_portfolio_value < self.bankrupt_threshold:\n","            terminated = True\n","            reward = -20.0 # Sehr harte Strafe\n","            self.current_portfolio_Value = 0.0\n","\n","        # Info-Dict für Logging (W&B)\n","        info = {\n","            'portfolio_value': float(self.current_portfolio_value),\n","            'prev_portfolio_value': float(prev_portfolio_value),\n","            'portfolio_value_after_costs': float(portfolio_return_after_costs),\n","            'reward': float(reward),\n","            'transaction_costs': float(cost * prev_portfolio_value),\n","            'turnover': float(turnover),\n","            'is_bankrupt': bool(self.current_portfolio_value < self.bankrupt_threshold)\n","        }\n","\n","        # (Gewichte-Schleife ist auskommentiert für sauberes Logging)\n","        # for i, w in enumerate(self.current_weights):\n","        #      info[f'weight_{i}'] = float(w)\n","\n","        return self._get_obs(), reward, terminated, truncated, info\n","\n","\n","    def reset(self, seed=None, options=None):\n","        super().reset(seed=seed)\n","\n","        self.current_step = self.lookback_window\n","        self.current_portfolio_value = self.initial_capital\n","        self.peak_portfolio_value = self.initial_capital\n","\n","        self.current_weights = np.zeros(self.N_ASSETS_PLUS_CASH, dtype=np.float32)\n","        self.current_weights[0] = 1.0\n","\n","        return self._get_obs(), {}\n","\n","    def render(self):\n","        pass\n","\n","    def close(self):\n","        pass"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cHB83j3HGtIS","executionInfo":{"status":"aborted","timestamp":1763630818444,"user_tz":-60,"elapsed":91521,"user":{"displayName":"Christoph Nachname","userId":"04640672557127032436"}}},"outputs":[],"source":["## 5. Custom DRL-Architektur (Extractor)\n","\n","class CustomCombinedExtractor(BaseFeaturesExtractor):\n","    \"\"\"\n","    Kombinierter Feature Extractor (LSTM + MLP) für den MultiInputPolicy.\n","    Nimmt 'features' (für LSTM) und 'portfolio_weights' (für MLP) entgegen.\n","    \"\"\"\n","    def __init__(self, observation_space: gym.spaces.Dict, features_dim: int = 128):\n","        # 'features_dim' ist die finale Output-Dimension, die an die Policy/Value-Köpfe geht\n","        super().__init__(observation_space, features_dim)\n","        print(f\"CustomCombinedExtractor (Typ: LSTM, Hidden: {features_dim}) initialisiert.\")\n","\n","        # --- 1. Pfad für sequenzielle Features (LSTM) ---\n","        feature_space = observation_space[\"features\"]\n","        self.feature_input_dim = feature_space.shape[1] * feature_space.shape[2]\n","        self.lookback_window = feature_space.shape[0]\n","\n","        self.lstm = nn.LSTM(\n","            input_size=self.feature_input_dim,\n","            hidden_size=64, # LSTM-Output-Größe\n","            num_layers=1,\n","            batch_first=True\n","        )\n","\n","        # --- 2. Pfad für Portfolio-Gewichte (MLP) ---\n","        weights_space = observation_space[\"portfolio_weights\"]\n","        self.weights_input_dim = weights_space.shape[0] # (N_Assets + Cash)\n","\n","        self.weights_mlp = nn.Sequential(\n","            nn.Linear(self.weights_input_dim, 32), # MLP-Output-Größe\n","            nn.ReLU()\n","        )\n","\n","        # --- 3. Kombinierte Output-Schicht (Fusion Layer) ---\n","        combined_input_dim = 64 + 32\n","\n","        self.final_fc = nn.Sequential(\n","            nn.Linear(combined_input_dim, features_dim),\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, observations: th.Tensor) -> th.Tensor:\n","        # --- 1. LSTM-Pfad ---\n","        features_obs = observations[\"features\"]\n","        batch_size = features_obs.shape[0]\n","        lstm_input = features_obs.view(batch_size, self.lookback_window, self.feature_input_dim)\n","        _, (hidden_state, _) = self.lstm(lstm_input)\n","        lstm_out = hidden_state.squeeze(0)\n","\n","        # --- 2. MLP-Pfad ---\n","        weights_obs = observations[\"portfolio_weights\"]\n","        weights_out = self.weights_mlp(weights_obs)\n","\n","        # --- 3. Fusion ---\n","        combined = th.cat((lstm_out, weights_out), dim=1)\n","\n","        # --- 4. Finaler Output ---\n","        return self.final_fc(combined)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DbqC7YqgDF74","executionInfo":{"status":"aborted","timestamp":1763630818446,"user_tz":-60,"elapsed":91520,"user":{"displayName":"Christoph Nachname","userId":"04640672557127032436"}}},"outputs":[],"source":["## 6. Training Starten (Linearer Lauf)\n","\n","# --- 1. Erstelle Vektorisierte Environments ---\n","# (Dieser Code MUSS nach der Definition von 'PortfolioEnv' in Zelle 4 ausgeführt werden)\n","\n","def create_env_factory(data, env_kwargs):\n","    \"\"\"Helferfunktion zur Erstellung einer Environment-Factory.\"\"\"\n","    def _init():\n","        # 'PortfolioEnv' ist jetzt im globalen Scope definiert\n","        return PortfolioEnv(data_df=data, **env_kwargs)\n","    return _init\n","\n","print(f\"Erstelle {NUM_CPU_CORES} parallele TRAIN-Umgebungen...\")\n","# ##################################################################\n","# ### KORREKTUR: Erzwinge DummyVecEnv für TRAIN-Umgebung ###\n","# ##################################################################\n","# Um sicherzustellen, dass die Trainingsumgebung ebenfalls ein DummyVecEnv ist\n","# (wie die Eval-Umgebung) und um Multiprocessing-Probleme auszuschließen,\n","# erstellen wir sie hier explizit als DummyVecEnv.\n","train_env = DummyVecEnv([create_env_factory(train_data, ENV_KWARGS) for _ in range(NUM_CPU_CORES)])\n","# ##################################################################\n","\n","\n","print(\"Erstelle EVAL-Umgebungs-Fabrik...\")\n","eval_env_factory = create_env_factory(eval_data, ENV_KWARGS)\n","\n","print(\"Erstelle (vektorisierte) EVAL-Umgebung...\")\n","# EvalCallback erwartet eine instanziierte VecEnv, keine Factory-Funktion.\n","# Wir nutzen DummyVecEnv (ein einzelner Prozess) für die Evaluation.\n","eval_env = DummyVecEnv([eval_env_factory])\n","\n","print(\"Environments erfolgreich erstellt.\")\n","\n","\n","# --- 2. Eindeutige Run-ID und Pfade ---\n","# (Holt sich MODEL_SAVE_PATH aus Zelle 3)\n","run_id = str(uuid.uuid4()).split('-')[-1]\n","run_name = f\"PPO_LSTM_Run_{run_id}\"\n","\n","MODEL_RUN_PATH = os.path.join(MODEL_SAVE_PATH, run_name)\n","BEST_MODEL_PATH = os.path.join(MODEL_RUN_PATH, \"best_model\")\n","os.makedirs(MODEL_RUN_PATH, exist_ok=True)\n","\n","print(f\"Starte neuen Lauf: {run_name}\")\n","print(f\"Modelle werden gespeichert in: {MODEL_RUN_PATH}\")\n","\n","# --- 3. W&B initialisieren (Nur EINMAL) ---\n","# (Holt sich WANDB_... und PPO_PARAMS_CONFIG aus Zelle 3)\n","run = wandb.init(\n","    project=WANDB_PROJECT_NAME,\n","    entity=WANDB_ENTITY,\n","    name=run_name,\n","    config=PPO_PARAMS_CONFIG, # Aus Zelle 3.8\n","    sync_tensorboard=True,\n","    monitor_gym=True,\n","    save_code=True,\n",")\n","\n","# --- 4. PPO-Modellparameter definieren ---\n","# (Holt sich PPO_PARAMS_CONFIG, NN_ACTIVATIONS, OPTIMIZERS aus Zelle 3)\n","policy_kwargs = dict(\n","    features_extractor_class=CustomCombinedExtractor,\n","    features_extractor_kwargs=dict(features_dim=PPO_PARAMS_CONFIG[\"features_dim\"]),\n","    net_arch=dict(\n","        pi=[PPO_PARAMS_CONFIG[\"policy_width\"]] * PPO_PARAMS_CONFIG[\"policy_depth\"],\n","        vf=[PPO_PARAMS_CONFIG[\"value_width\"]] * PPO_PARAMS_CONFIG[\"value_depth\"]\n","    ),\n","    activation_fn=NN_ACTIVATIONS[PPO_PARAMS_CONFIG[\"activation_fn\"]],\n","    optimizer_class=OPTIMIZERS[PPO_PARAMS_CONFIG[\"optimizer\"]],\n",")\n","\n","ppo_params = {\n","    \"learning_rate\": PPO_PARAMS_CONFIG[\"learning_rate\"],\n","    \"n_steps\": PPO_PARAMS_CONFIG[\"n_steps\"],\n","    \"batch_size\": PPO_PARAMS_CONFIG[\"batch_size\"],\n","    \"n_epochs\": PPO_PARAMS_CONFIG[\"n_epochs\"],\n","    \"gamma\": PPO_PARAMS_CONFIG[\"gamma\"],\n","    \"gae_lambda\": PPO_PARAMS_CONFIG[\"gae_lambda\"],\n","    \"clip_range\": PPO_PARAMS_CONFIG[\"clip_range\"],\n","    \"ent_coef\": PPO_PARAMS_CONFIG[\"ent_coef\"],\n","    \"vf_coef\": PPO_PARAMS_CONFIG[\"vf_coef\"],\n","    \"max_grad_norm\": PPO_PARAMS_CONFIG[\"max_grad_norm\"],\n","    \"policy_kwargs\": policy_kwargs\n","}\n","\n","# --- 5. Callbacks aufsetzen (Nur die zwei Standard-Callbacks) ---\n","\n","# Callback 1: Standard SB3 Eval Callback\n","# (Wir übergeben jetzt 'eval_env', nicht 'eval_env_factory')\n","eval_callback = EvalCallback(\n","    eval_env,\n","    best_model_save_path=BEST_MODEL_PATH,\n","    log_path=MODEL_RUN_PATH,\n","    eval_freq=max(EVAL_FREQ // NUM_CPU_CORES, 1),\n","    n_eval_episodes=N_EVAL_EPISODES,\n","    deterministic=True,\n","    render=False\n",")\n","\n","# Callback 2: Die OFFIZIELLE W&B Callback\n","official_wandb_cb = OfficialWandbCallback(\n","    gradient_save_freq=0,\n","    model_save_path=MODEL_RUN_PATH,\n","    model_save_freq=0, # EvalCallback speichert das beste Modell\n","    verbose=0\n",")\n","\n","# Alle Callbacks kombinieren\n","callback_list = CallbackList([\n","    eval_callback,\n","    official_wandb_cb,\n","])\n","\n","# --- 6. Modell erstellen und trainieren ---\n","# (Holt sich LOG_BASE_PATH, DEVICE, DEFAULT_SEED aus Zelle 3)\n","model = PPO(\n","    \"MultiInputPolicy\",\n","    train_env,\n","    tensorboard_log=f\"{LOG_BASE_PATH}/tensorboard/{run.id}\",\n","    verbose=1,\n","    device=DEVICE,\n","    seed=DEFAULT_SEED,\n","    **ppo_params\n",")\n","\n","print(f\"\\n--- Starte Training für Run {run_name} ({TOTAL_TIMESTEPS} Schritte) ---\")\n","\n","try:\n","    model.learn(\n","        total_timesteps=TOTAL_TIMESTEPS, # Aus Zelle 3.7\n","        callback=callback_list,\n","        progress_bar=True\n","    )\n","    print(\"\\n--- Training erfolgreich abgeschlossen. ---\")\n","\n","except Exception as e:\n","    print(f\"\\n--- FATALER FEHLER während des Trainings ---\")\n","    traceback.print_exc()\n","\n","finally:\n","    # Den W&B-Lauf sauber beenden\n","    run.finish()\n","    print(f\"--- Run {run.name} beendet und W&B-Sync abgeschlossen. ---\")"]},{"cell_type":"markdown","metadata":{"id":"d785d17d"},"source":["# Task\n","Add debug print statements at various critical points within the `step` method of the `PortfolioEnv` class (Cell 4) to monitor the execution flow, then re-run the training cell (Cell 6) and analyze the debug output to identify the exact location or cause of the training hang."]},{"cell_type":"markdown","metadata":{"id":"dfb9f4c6"},"source":["## Add Debug Prints to PortfolioEnv\n","\n","### Subtask:\n","Add `print` statements at various critical points within the `step` method of the `PortfolioEnv` class (Cell 4) to monitor the execution flow. This will help determine if the environment's `step` method is being called and where it might be getting stuck.\n"]},{"cell_type":"markdown","metadata":{"id":"c94c0fb0"},"source":["**Reasoning**:\n","To implement the subtask, I need to modify the `PortfolioEnv` class by adding the requested debug print statements within its `step` method. This will involve updating the code in cell `GvvxJD8DE1FE`.\n","\n"]},{"cell_type":"code","metadata":{"id":"33d46c22","executionInfo":{"status":"aborted","timestamp":1763630818447,"user_tz":-60,"elapsed":91517,"user":{"displayName":"Christoph Nachname","userId":"04640672557127032436"}}},"source":["## 4. Custom Portfolio Environment (Gymnasium)\n","\n","class PortfolioEnv(gym.Env):\n","    \"\"\"\n","    Custom Environment für Portfolio-Optimierung (basierend auf Stable_Baseline_V4).\n","    - Observation Space: Dict (Features [LSTM Input], Portfolio Weights [MLP Input])\n","    - Action Space: Box (Logits für eine Dirichlet-Verteilung)\n","    - Reward: Log-Return des Portfolios.\n","    - Termination: Max Drawdown oder Bankrott.\n","    \"\"\"\n","    metadata = {'render_modes': ['human'], 'render_fps': 30}\n","\n","    def __init__(self, data_df: pd.DataFrame, **kwargs):\n","        super(PortfolioEnv, self).__init__()\n","\n","        self.data = data_df\n","        # Price Ratios sind exp(Log Returns).\n","        self.price_ratios = np.exp(data_df.loc[:, 'LogReturns'].values)\n","\n","        # Konfiguration (wird von Zelle 3 übergeben)\n","        self.initial_capital = kwargs.get(\"initial_capital\", 10000.0)\n","        self.max_drawdown = kwargs.get(\"max_drawdown\", 0.5)\n","        self.transaction_cost_pct = kwargs.get(\"transaction_cost_pct\", 0.001)\n","        self.lookback_window = kwargs.get(\"lookback_window\", 10)\n","        self.bankrupt_threshold = kwargs.get(\"bankrupt_threshold\", 1.0)\n","\n","        # Dimensionen (aus Zelle 3 geladen)\n","        self.num_assets = NUM_ASSETS\n","        self.num_features = NUM_FEATURES\n","        self.N_ASSETS_PLUS_CASH = N_ASSETS_PLUS_CASH # Cash ist Index 0\n","\n","        # State\n","        self.current_step = self.lookback_window # Start nach Lookback-Window\n","        self.max_steps = len(data_df) - 1\n","        self.current_portfolio_value = self.initial_capital\n","        self.peak_portfolio_value = self.initial_capital\n","\n","        # Initialisierung der Gewichte (Cash = 100%)\n","        self.current_weights = np.zeros(self.N_ASSETS_PLUS_CASH, dtype=np.float32)\n","        self.current_weights[0] = 1.0\n","\n","        # Action Space (Logits, die zu Alphas werden)\n","        self.action_space = spaces.Box(low=-100.0, high=100.0, shape=(self.N_ASSETS_PLUS_CASH,), dtype=np.float32)\n","\n","\n","        # Observation Space (Input für Actor-Critic Netzwerke)\n","        self.observation_space = spaces.Dict({\n","            # Features (L, N_Assets, N_Features) -> LSTM-Input\n","            'features': spaces.Box(low=-np.inf, high=np.inf,\n","                                   shape=(self.lookback_window, self.num_assets, self.num_features),\n","                                   dtype=np.float32),\n","            # Portfolio Weights (N_Assets + Cash) -> MLP-Input\n","            'portfolio_weights': spaces.Box(low=0.0, high=1.0,\n","                                            shape=(self.N_ASSETS_PLUS_CASH,),\n","                                            dtype=np.float32)\n","        })\n","\n","        # --- Daten-Validierung (Price Ratios) ---\n","        max_ratio = np.max(self.price_ratios)\n","        min_ratio = np.min(self.price_ratios)\n","        if np.isinf(max_ratio) or np.isinf(min_ratio) or np.isnan(max_ratio) or np.isnan(min_ratio):\n","             print(\"\\n\" + \"=\"*70)\n","             print(f\"FATALER FEHLER: 'inf' oder 'NaN' in price_ratios gefunden.\")\n","             print(f\"Max: {max_ratio}, Min: {min_ratio}\")\n","             print(\"Bitte Notebook 02 ausführen und LogReturns clippen!\")\n","             print(\"=\"*70 + \"\\n\")\n","             raise ValueError(\"inf/nan in price_ratios\")\n","\n","        print(\"--- Daten-Validierung (Price Ratios) ---\")\n","        print(f\"Max Price Ratio: {max_ratio}\")\n","        print(f\"Min Price Ratio: {min_ratio}\")\n","        print(f\"Mean Price Ratio: {np.mean(self.price_ratios)}\")\n","        print(f\"NaNs in Price Ratios: {np.isnan(self.price_ratios).sum()}\")\n","        print(\"------------------------------------------\")\n","\n","\n","    def _get_obs(self):\n","        start = self.current_step - self.lookback_window\n","        end = self.current_step\n","        feature_history_df = self.data.iloc[start:end]\n","\n","        # (L, N_Features, N_Assets) -> (L, N_Assets, N_Features)\n","        feature_history = feature_history_df.values.reshape(\n","            self.lookback_window, self.num_features, self.num_assets\n","        ).swapaxes(1, 2)\n","\n","        return {\n","            'features': feature_history.astype(np.float32),\n","            'portfolio_weights': self.current_weights.astype(np.float32)\n","        }\n","\n","\n","    def step(self, action: np.ndarray):\n","        print(f\"[DEBUG] Step method called at step {self.current_step}\")\n","\n","        # ##################################################################\n","        # ### HIER IST DER FIX FÜR DEN SLOWDOWN ###\n","        # ##################################################################\n","        #\n","        # ALT (Instabil und langsam):\n","        # alpha = np.log(1.0 + np.exp(action)) + 1e-6\n","        #\n","        # NEU (Numerisch stabil und schnell):\n","        # np.logaddexp(0, action) ist äquivalent zu log(exp(0) + exp(action))\n","        # = log(1 + exp(action))\n","        alpha = np.logaddexp(0, action) + 1e-6\n","        print(f\"[DEBUG] Alpha calculated: {alpha[:5]}...\")\n","        # ##################################################################\n","\n","        # 1. Aktion zu Portfolio-Gewichten (Dirichlet)\n","        new_weights = alpha / np.sum(alpha)\n","        print(f\"[DEBUG] New weights calculated: {new_weights[:5]}...\")\n","\n","\n","        # 2. Portfolio-Wertentwicklung berechnen\n","        asset_ratios_today = self.price_ratios[self.current_step]\n","        price_ratios_with_cash = np.insert(asset_ratios_today, 0, 1.0)\n","        portfolio_return_before_costs = np.dot(new_weights, price_ratios_with_cash)\n","\n","        # 3. Transaktionskosten berechnen\n","        turnover = np.sum(np.abs(new_weights[1:] - self.current_weights[1:]))\n","        cost = turnover * self.transaction_cost_pct\n","\n","        # 4. Finaler Return und neuer Portfolio-Wert\n","        portfolio_return_after_costs = portfolio_return_before_costs - cost\n","        portfolio_return_after_costs = max(portfolio_return_after_costs, 0)\n","\n","        prev_portfolio_value = self.current_portfolio_value\n","        self.current_portfolio_value *= portfolio_return_after_costs\n","        print(f\"[DEBUG] Portfolio value updated to: {self.current_portfolio_value:.2f}\")\n","\n","        # 5. Reward (Log-Return)\n","        reward = np.log(max(portfolio_return_after_costs, 1e-10))\n","\n","        # 6. Metriken und State-Update\n","        self.current_step += 1\n","        self.current_weights = new_weights\n","\n","        # Drawdown-Berechnung\n","        self.peak_portfolio_value = max(self.peak_portfolio_value, self.current_portfolio_value)\n","        current_drawdown = (self.peak_portfolio_value - self.current_portfolio_value) / self.peak_portfolio_value\n","\n","        # 7. Termination und Truncation\n","        terminated = False\n","        truncated = False\n","\n","        print(f\"[DEBUG] Checking termination conditions...\")\n","        if self.current_step >= self.max_steps:\n","            terminated = True\n","        if current_drawdown > self.max_drawdown:\n","            terminated = True\n","            reward = -10.0 # Harte Strafe\n","        if self.current_portfolio_value < self.bankrupt_threshold:\n","            terminated = True\n","            reward = -20.0 # Sehr harte Strafe\n","            self.current_portfolio_value = 0.0\n","\n","        # Info-Dict für Logging (W&B)\n","        info = {\n","            'portfolio_value': float(self.current_portfolio_value),\n","            'prev_portfolio_value': float(prev_portfolio_value),\n","            'portfolio_value_after_costs': float(portfolio_return_after_costs),\n","            'reward': float(reward),\n","            'transaction_costs': float(cost * prev_portfolio_value),\n","            'turnover': float(turnover),\n","            'is_bankrupt': bool(self.current_portfolio_value < self.bankrupt_threshold)\n","        }\n","\n","        # (Gewichte-Schleife ist auskommentiert für sauberes Logging)\n","        # for i, w in enumerate(self.current_weights):\n","        #      info[f'weight_{i}'] = float(w)\n","\n","        print(f\"[DEBUG] Step method returning. Reward: {reward:.4f}, Terminated: {terminated}\")\n","        return self._get_obs(), reward, terminated, truncated, info\n","\n","\n","    def reset(self, seed=None, options=None):\n","        super().reset(seed=seed)\n","\n","        self.current_step = self.lookback_window\n","        self.current_portfolio_value = self.initial_capital\n","        self.peak_portfolio_value = self.initial_capital\n","\n","        self.current_weights = np.zeros(self.N_ASSETS_PLUS_CASH, dtype=np.float32)\n","        self.current_weights[0] = 1.0\n","\n","        return self._get_obs(), {}\n","\n","    def render(self):\n","        pass\n","\n","    def close(self):\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"638e0f5d"},"source":["## Re-run Training Cell\n","\n","### Subtask:\n","Execute the training cell (Cell 6) again to observe the new debug output from the `PortfolioEnv`. This will provide insights into whether the environment is processing steps and where the hang occurs.\n"]},{"cell_type":"markdown","metadata":{"id":"44e70366"},"source":["**Reasoning**:\n","To observe the debug output from the `PortfolioEnv`'s `step` method, I need to re-run the training cell (Cell 6) that initializes and trains the PPO model.\n","\n"]},{"cell_type":"code","metadata":{"id":"78de98ba","executionInfo":{"status":"aborted","timestamp":1763630818539,"user_tz":-60,"elapsed":2,"user":{"displayName":"Christoph Nachname","userId":"04640672557127032436"}}},"source":["## 6. Training Starten (Linearer Lauf)\n","\n","# --- 1. Erstelle Vektorisierte Environments ---\n","# (Dieser Code MUSS nach der Definition von 'PortfolioEnv' in Zelle 4 ausgeführt werden)\n","\n","def create_env_factory(data, env_kwargs):\n","    \"\"\"Helferfunktion zur Erstellung einer Environment-Factory.\"\"\"\n","    def _init():\n","        # 'PortfolioEnv' ist jetzt im globalen Scope definiert\n","        return PortfolioEnv(data_df=data, **env_kwargs)\n","    return _init\n","\n","print(f\"Erstelle {NUM_CPU_CORES} parallele TRAIN-Umgebungen...\")\n","# ##################################################################\n","# ### KORREKTUR: Erzwinge DummyVecEnv für TRAIN-Umgebung ###\n","# ##################################################################\n","# Um sicherzustellen, dass die Trainingsumgebung ebenfalls ein DummyVecEnv ist\n","# (wie die Eval-Umgebung) und um Multiprocessing-Probleme auszuschließen,\n","# erstellen wir sie hier explizit als DummyVecEnv.\n","train_env = DummyVecEnv([create_env_factory(train_data, ENV_KWARGS) for _ in range(NUM_CPU_CORES)])\n","# ##################################################################\n","\n","\n","print(\"Erstelle EVAL-Umgebungs-Fabrik...\")\n","eval_env_factory = create_env_factory(eval_data, ENV_KWARGS)\n","\n","print(\"Erstelle (vektorisierte) EVAL-Umgebung...\")\n","# EvalCallback erwartet eine instanziierte VecEnv, keine Factory-Funktion.\n","# Wir nutzen DummyVecEnv (ein einzelner Prozess) für die Evaluation.\n","eval_env = DummyVecEnv([eval_env_factory])\n","\n","print(\"Environments erfolgreich erstellt.\")\n","\n","\n","# --- 2. Eindeutige Run-ID und Pfade ---\n","# (Holt sich MODEL_SAVE_PATH aus Zelle 3)\n","run_id = str(uuid.uuid4()).split('-')[-1]\n","run_name = f\"PPO_LSTM_Run_{run_id}\"\n","\n","MODEL_RUN_PATH = os.path.join(MODEL_SAVE_PATH, run_name)\n","BEST_MODEL_PATH = os.path.join(MODEL_RUN_PATH, \"best_model\")\n","os.makedirs(MODEL_RUN_PATH, exist_ok=True)\n","\n","print(f\"Starte neuen Lauf: {run_name}\")\n","print(f\"Modelle werden gespeichert in: {MODEL_RUN_PATH}\")\n","\n","# --- 3. W&B initialisieren (Nur EINMAL) ---\n","# (Holt sich WANDB_... und PPO_PARAMS_CONFIG aus Zelle 3)\n","run = wandb.init(\n","    project=WANDB_PROJECT_NAME,\n","    entity=WANDB_ENTITY,\n","    name=run_name,\n","    config=PPO_PARAMS_CONFIG, # Aus Zelle 3.8\n","    sync_tensorboard=True,\n","    monitor_gym=True,\n","    save_code=True,\n",")\n","\n","# --- 4. PPO-Modellparameter definieren ---\n","# (Holt sich PPO_PARAMS_CONFIG, NN_ACTIVATIONS, OPTIMIZERS aus Zelle 3)\n","policy_kwargs = dict(\n","    features_extractor_class=CustomCombinedExtractor,\n","    features_extractor_kwargs=dict(features_dim=PPO_PARAMS_CONFIG[\"features_dim\"]),\n","    net_arch=dict(\n","        pi=[PPO_PARAMS_CONFIG[\"policy_width\"]] * PPO_PARAMS_CONFIG[\"policy_depth\"],\n","        vf=[PPO_PARAMS_CONFIG[\"value_width\"]] * PPO_PARAMS_CONFIG[\"value_depth\"]\n","    ),\n","    activation_fn=NN_ACTIVATIONS[PPO_PARAMS_CONFIG[\"activation_fn\"]],\n","    optimizer_class=OPTIMIZERS[PPO_PARAMS_CONFIG[\"optimizer\"]],\n",")\n","\n","ppo_params = {\n","    \"learning_rate\": PPO_PARAMS_CONFIG[\"learning_rate\"],\n","    \"n_steps\": PPO_PARAMS_CONFIG[\"n_steps\"],\n","    \"batch_size\": PPO_PARAMS_CONFIG[\"batch_size\"],\n","    \"n_epochs\": PPO_PARAMS_CONFIG[\"n_epochs\"],\n","    \"gamma\": PPO_PARAMS_CONFIG[\"gamma\"],\n","    \"gae_lambda\": PPO_PARAMS_CONFIG[\"gae_lambda\"],\n","    \"clip_range\": PPO_PARAMS_CONFIG[\"clip_range\"],\n","    \"ent_coef\": PPO_PARAMS_CONFIG[\"ent_coef\"],\n","    \"vf_coef\": PPO_PARAMS_CONFIG[\"vf_coef\"],\n","    \"max_grad_norm\": PPO_PARAMS_CONFIG[\"max_grad_norm\"],\n","    \"policy_kwargs\": policy_kwargs\n","}\n","\n","# --- 5. Callbacks aufsetzen (Nur die zwei Standard-Callbacks) ---\n","\n","# Callback 1: Standard SB3 Eval Callback\n","# (Wir übergeben jetzt 'eval_env', nicht 'eval_env_factory')\n","eval_callback = EvalCallback(\n","    eval_env,\n","    best_model_save_path=BEST_MODEL_PATH,\n","    log_path=MODEL_RUN_PATH,\n","    eval_freq=max(EVAL_FREQ // NUM_CPU_CORES, 1),\n","    n_eval_episodes=N_EVAL_EPISODES,\n","    deterministic=True,\n","    render=False\n",")\n","\n","# Callback 2: Die OFFIZIELLE W&B Callback\n","official_wandb_cb = OfficialWandbCallback(\n","    gradient_save_freq=0,\n","    model_save_path=MODEL_RUN_PATH,\n","    model_save_freq=0, # EvalCallback speichert das beste Modell\n","    verbose=0\n",")\n","\n","# Alle Callbacks kombinieren\n","callback_list = CallbackList([\n","    eval_callback,\n","    official_wandb_cb,\n","])\n","\n","# --- 6. Modell erstellen und trainieren ---\n","# (Holt sich LOG_BASE_PATH, DEVICE, DEFAULT_SEED aus Zelle 3)\n","model = PPO(\n","    \"MultiInputPolicy\",\n","    train_env,\n","    tensorboard_log=f\"{LOG_BASE_PATH}/tensorboard/{run.id}\",\n","    verbose=1,\n","    device=DEVICE,\n","    seed=DEFAULT_SEED,\n","    **ppo_params\n",")\n","\n","print(f\"\\n--- Starte Training für Run {run_name} ({TOTAL_TIMESTEPS} Schritte) ---\")\n","\n","try:\n","    model.learn(\n","        total_timesteps=TOTAL_TIMESTEPS, # Aus Zelle 3.7\n","        callback=callback_list,\n","        progress_bar=True\n","    )\n","    print(\"\\n--- Training erfolgreich abgeschlossen. ---\")\n","\n","except Exception as e:\n","    print(f\"\\n--- FATALER FEHLER während des Trainings ---\")\n","    traceback.print_exc()\n","\n","finally:\n","    # Den W&B-Lauf sauber beenden\n","    run.finish()\n","    print(f\"--- Run {run.name} beendet und W&B-Sync abgeschlossen. ---\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"35022674"},"source":["**Reasoning**:\n","The subtask explicitly asks to re-run cell 6 to observe the debug output. The previous step already modified the `PortfolioEnv` in cell 4, so re-running cell 6 will execute the training with the added debug statements.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"2dadd470","executionInfo":{"status":"error","timestamp":1763630839534,"user_tz":-60,"elapsed":79,"user":{"displayName":"Christoph Nachname","userId":"04640672557127032436"}},"outputId":"276b090f-bfbf-47f1-fc18-68edd8e49498"},"source":["## 6. Training Starten (Linearer Lauf)\n","\n","# --- 1. Erstelle Vektorisierte Environments ---\n","# (Dieser Code MUSS nach der Definition von 'PortfolioEnv' in Zelle 4 ausgeführt werden)\n","\n","def create_env_factory(data, env_kwargs):\n","    \"\"\"Helferfunktion zur Erstellung einer Environment-Factory.\"\"\"\n","    def _init():\n","        # 'PortfolioEnv' ist jetzt im globalen Scope definiert\n","        return PortfolioEnv(data_df=data, **env_kwargs)\n","    return _init\n","\n","print(f\"Erstelle {NUM_CPU_CORES} parallele TRAIN-Umgebungen...\")\n","# ##################################################################\n","# ### KORREKTUR: Erzwinge DummyVecEnv für TRAIN-Umgebung ###\n","# ##################################################################\n","# Um sicherzustellen, dass die Trainingsumgebung ebenfalls ein DummyVecEnv ist\n","# (wie die Eval-Umgebung) und um Multiprocessing-Probleme auszuschließen,\n","# erstellen wir sie hier explizit als DummyVecEnv.\n","train_env = DummyVecEnv([create_env_factory(train_data, ENV_KWARGS) for _ in range(NUM_CPU_CORES)])\n","# ##################################################################\n","\n","\n","print(\"Erstelle EVAL-Umgebungs-Fabrik...\")\n","eval_env_factory = create_env_factory(eval_data, ENV_KWARGS)\n","\n","print(\"Erstelle (vektorisierte) EVAL-Umgebung...\")\n","# EvalCallback erwartet eine instanziierte VecEnv, keine Factory-Funktion.\n","# Wir nutzen DummyVecEnv (ein einzelner Prozess) für die Evaluation.\n","eval_env = DummyVecEnv([eval_env_factory])\n","\n","print(\"Environments erfolgreich erstellt.\")\n","\n","\n","# --- 2. Eindeutige Run-ID und Pfade ---\n","# (Holt sich MODEL_SAVE_PATH aus Zelle 3)\n","run_id = str(uuid.uuid4()).split('-')[-1]\n","run_name = f\"PPO_LSTM_Run_{run_id}\"\n","\n","MODEL_RUN_PATH = os.path.join(MODEL_SAVE_PATH, run_name)\n","BEST_MODEL_PATH = os.path.join(MODEL_RUN_PATH, \"best_model\")\n","os.makedirs(MODEL_RUN_PATH, exist_ok=True)\n","\n","print(f\"Starte neuen Lauf: {run_name}\")\n","print(f\"Modelle werden gespeichert in: {MODEL_RUN_PATH}\")\n","\n","# --- 3. W&B initialisieren (Nur EINMAL) ---\n","# (Holt sich WANDB_... und PPO_PARAMS_CONFIG aus Zelle 3)\n","run = wandb.init(\n","    project=WANDB_PROJECT_NAME,\n","    entity=WANDB_ENTITY,\n","    name=run_name,\n","    config=PPO_PARAMS_CONFIG, # Aus Zelle 3.8\n","    sync_tensorboard=True,\n","    monitor_gym=True,\n","    save_code=True,\n",")\n","\n","# --- 4. PPO-Modellparameter definieren ---\n","# (Holt sich PPO_PARAMS_CONFIG, NN_ACTIVATIONS, OPTIMIZERS aus Zelle 3)\n","policy_kwargs = dict(\n","    features_extractor_class=CustomCombinedExtractor,\n","    features_extractor_kwargs=dict(features_dim=PPO_PARAMS_CONFIG[\"features_dim\"]),\n","    net_arch=dict(\n","        pi=[PPO_PARAMS_CONFIG[\"policy_width\"]] * PPO_PARAMS_CONFIG[\"policy_depth\"],\n","        vf=[PPO_PARAMS_CONFIG[\"value_width\"]] * PPO_PARAMS_CONFIG[\"value_depth\"]\n","    ),\n","    activation_fn=NN_ACTIVATIONS[PPO_PARAMS_CONFIG[\"activation_fn\"]],\n","    optimizer_class=OPTIMIZERS[PPO_PARAMS_CONFIG[\"optimizer\"]],\n",")\n","\n","ppo_params = {\n","    \"learning_rate\": PPO_PARAMS_CONFIG[\"learning_rate\"],\n","    \"n_steps\": PPO_PARAMS_CONFIG[\"n_steps\"],\n","    \"batch_size\": PPO_PARAMS_CONFIG[\"batch_size\"],\n","    \"n_epochs\": PPO_PARAMS_CONFIG[\"n_epochs\"],\n","    \"gamma\": PPO_PARAMS_CONFIG[\"gamma\"],\n","    \"gae_lambda\": PPO_PARAMS_CONFIG[\"gae_lambda\"],\n","    \"clip_range\": PPO_PARAMS_CONFIG[\"clip_range\"],\n","    \"ent_coef\": PPO_PARAMS_CONFIG[\"ent_coef\"],\n","    \"vf_coef\": PPO_PARAMS_CONFIG[\"vf_coef\"],\n","    \"max_grad_norm\": PPO_PARAMS_CONFIG[\"max_grad_norm\"],\n","    \"policy_kwargs\": policy_kwargs\n","}\n","\n","# --- 5. Callbacks aufsetzen (Nur die zwei Standard-Callbacks) ---\n","\n","# Callback 1: Standard SB3 Eval Callback\n","# (Wir übergeben jetzt 'eval_env', nicht 'eval_env_factory')\n","eval_callback = EvalCallback(\n","    eval_env,\n","    best_model_save_path=BEST_MODEL_PATH,\n","    log_path=MODEL_RUN_PATH,\n","    eval_freq=max(EVAL_FREQ // NUM_CPU_CORES, 1),\n","    n_eval_episodes=N_EVAL_EPISODES,\n","    deterministic=True,\n","    render=False\n",")\n","\n","# Callback 2: Die OFFIZIELLE W&B Callback\n","official_wandb_cb = OfficialWandbCallback(\n","    gradient_save_freq=0,\n","    model_save_path=MODEL_RUN_PATH,\n","    model_save_freq=0, # EvalCallback speichert das beste Modell\n","    verbose=0\n",")\n","\n","# Alle Callbacks kombinieren\n","callback_list = CallbackList([\n","    eval_callback,\n","    official_wandb_cb,\n","])\n","\n","# --- 6. Modell erstellen und trainieren ---\n","# (Holt sich LOG_BASE_PATH, DEVICE, DEFAULT_SEED aus Zelle 3)\n","model = PPO(\n","    \"MultiInputPolicy\",\n","    train_env,\n","    tensorboard_log=f\"{LOG_BASE_PATH}/tensorboard/{run.id}\",\n","    verbose=1,\n","    device=DEVICE,\n","    seed=DEFAULT_SEED,\n","    **ppo_params\n",")\n","\n","print(f\"\\n--- Starte Training für Run {run_name} ({TOTAL_TIMESTEPS} Schritte) ---\")\n","\n","try:\n","    model.learn(\n","        total_timesteps=TOTAL_TIMESTEPS, # Aus Zelle 3.7\n","        callback=callback_list,\n","        progress_bar=True\n","    )\n","    print(\"\\n--- Training erfolgreich abgeschlossen. ---\")\n","\n","except Exception as e:\n","    print(f\"\\n--- FATALER FEHLER während des Trainings ---\")\n","    traceback.print_exc()\n","\n","finally:\n","    # Den W&B-Lauf sauber beenden\n","    run.finish()\n","    print(f\"--- Run {run.name} beendet und W&B-Sync abgeschlossen. ---\")"],"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'NUM_CPU_CORES' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2501149226.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Erstelle {NUM_CPU_CORES} parallele TRAIN-Umgebungen...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# ##################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# ### KORREKTUR: Erzwinge DummyVecEnv für TRAIN-Umgebung ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'NUM_CPU_CORES' is not defined"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1s0Cped5MHj8Lnef6mijNTjLULzbwdDRE","timestamp":1762620994425},{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/christophbieritz/ppo-stable-baseline-v2.e0d09903-048c-4ed6-b4b2-0352c7265933.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20251108/auto/storage/goog4_request&X-Goog-Date=20251108T060700Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=651bd69ccf890d4aef35ff5f0ed5871631ce648bcadfcd5bfeb3367cf72adf30fffbf3451176c7bc7c23603a2477953ab955d70842f359b891dc50c6f2d9174d578b4c53c0af0c2506d49e830fa3ec5141a2bcd6ed6c72faf1768ad5f10745b2556b303c5f15f80334e18555812e70a5f44cdc3cfb4a284d8380daf3ef38a1884a819b1fe031b4b2209aeccc1d0ee02265eb332a1241d9a9b5564560fd12ed3159f0e7b1181a903c8bebc16062b41e85b67e61b926e73d8e426700c723d319965d53778182a6f215f6a259dde0ddc642cd43358577b99040cf5dbc0c08f175f4d2d004df045cf21ca8fb4705a4e266ef0dda5c42ddbf613a2fb4a62d46714e29","timestamp":1762582068257}]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":8673648,"sourceId":13644596,"sourceType":"datasetVersion"}],"dockerImageVersionId":31153,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"}},"nbformat":4,"nbformat_minor":0}