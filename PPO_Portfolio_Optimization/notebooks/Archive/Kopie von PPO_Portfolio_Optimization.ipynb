{"cells":[{"cell_type":"markdown","id":"2e87c4ee","metadata":{"id":"2e87c4ee"},"source":["# Import der Libraries"]},{"cell_type":"code","execution_count":null,"id":"RzuH6dO8nAzu","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11264,"status":"ok","timestamp":1762368852227,"user":{"displayName":"Christoph Nachname","userId":"04640672557127032436"},"user_tz":-60},"id":"RzuH6dO8nAzu","outputId":"96411e96-1373-4394-ff65-33577200941c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.2)\n","Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.42.1)\n","Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"]}],"source":["!pip install wandb"]},{"cell_type":"code","execution_count":null,"id":"VxA21wSonJdH","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":280360,"status":"ok","timestamp":1762369132587,"user":{"displayName":"Christoph Nachname","userId":"04640672557127032436"},"user_tz":-60},"id":"VxA21wSonJdH","outputId":"bfee4e43-8cbe-4d2c-f081-57659691bbde"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchristoph-bieritz\u001b[0m (\u001b[33mcb-ml\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]}],"source":["!wandb login"]},{"cell_type":"code","execution_count":null,"id":"48eb4f5b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72181,"status":"ok","timestamp":1762369204779,"user":{"displayName":"Christoph Nachname","userId":"04640672557127032436"},"user_tz":-60},"id":"48eb4f5b","outputId":"e6c6b17f-d1d4-4bff-c9be-c1f7f843204a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["import os\n","import wandb\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import gymnasium as gym\n","import tensorflow_probability as tfp\n","import tensorflow.keras.losses as kls\n","\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate\n","from gymnasium import spaces\n","from google.colab import drive\n","from typing import List, Dict, Optional, Callable\n","\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"7373e7aa","metadata":{"id":"7373e7aa"},"outputs":[],"source":["csv_path = '/content/drive/MyDrive/Data/PPO_portfolio_optimization/DS1_Winner_B&H.csv'\n","return_cols = pd.read_csv('/content/drive/MyDrive/Data/PPO_portfolio_optimization/DS1_Winner_BH.csv',index_col=0).columns\n","# --- Hyperparameter fÃ¼r das Training ---\n","num_iterations = 250  # Wie viele Zyklen aus Sammeln & Trainieren\n","steps_per_rollout = 2048  # Schritte pro Datensammlung (BatchgrÃ¶ÃŸe)\n","num_ppo_epochs = 10  # Wie oft Ã¼ber denselben Batch trainieren\n","mini_batch_size = 512  # Mini-Batch-GrÃ¶ÃŸe fÃ¼r das Training\n","gamma = 0.99  # Discount-Faktor\n","lambda_gae = 0.95  # GAE-Parameter\n","learning_rate=0.001\n","\n","# --- PPO Hyperparameter ---\n","ppo_clip_epsilon = 0.2\n","vf_loss_coeff = 0.5\n","entropy_coeff = 0.01\n","\n","initial_balance = 10000.0\n","window_size = 30\n","transaction_cost_pct = 0.000\n","sharpe_window = 20\n","sharpe_ma_window = 10\n","\n","# --- Network Architecture ---\n","# TBD\n","\n","# --- Optimierer ---\n","optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","# --- Loss-Funktion (HIER DIE NEUE ZEILE HINZUFÃœGEN) ---\n","critic_loss_fn = tf.keras.losses.MeanSquaredError()\n"]},{"cell_type":"code","execution_count":null,"id":"PUN-ChweWX1A","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"elapsed":4223,"status":"ok","timestamp":1762369232295,"user":{"displayName":"Christoph Nachname","userId":"04640672557127032436"},"user_tz":-60},"id":"PUN-ChweWX1A","outputId":"1e7f0a07-166f-4f26-f48f-57580b34d0a7"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n","  | |_| | '_ \\/ _` / _` |  _/ -_)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchristoph-bieritz\u001b[0m (\u001b[33mcb-ml\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.22.2"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20251105_190030-4znxng5a</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/cb-ml/deep_learning_reinforcement_learning/runs/4znxng5a' target=\"_blank\">ruby-wave-75</a></strong> to <a href='https://wandb.ai/cb-ml/deep_learning_reinforcement_learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/cb-ml/deep_learning_reinforcement_learning' target=\"_blank\">https://wandb.ai/cb-ml/deep_learning_reinforcement_learning</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/cb-ml/deep_learning_reinforcement_learning/runs/4znxng5a' target=\"_blank\">https://wandb.ai/cb-ml/deep_learning_reinforcement_learning/runs/4znxng5a</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["def wandb_init():# Start a new wandb run to track this script.\n","  run = wandb.init(\n","      # Set the wandb entity where your project will be logged (generally your team name).\n","      entity=\"cb-ml\", # Changed to your entity\n","      # Set the wandb project where this run will be logged.\n","      project=\"deep_learning_reinforcement_learning\",# Make sure this project exists or change the name\n","      save_code=True,\n","      monitor_gym=True,\n","      reinit=True,\n","      # Track hyperparameters and run metadata.\n","      config={\n","          \"num_iterations\": num_iterations,\n","          \"steps_per_rollout\": steps_per_rollout,\n","          \"num_ppo_epochs\": num_ppo_epochs,\n","          \"mini_batch_size\": mini_batch_size,\n","          \"gamma\": gamma,\n","          \"lambda_gae\": lambda_gae,\n","          \"learning_rate\": learning_rate,\n","          \"ppo_clip_epsilon\": ppo_clip_epsilon,\n","          \"vf_loss_coeff\": vf_loss_coeff,\n","          \"entropy_coeff\": entropy_coeff,\n","          \"initial_balance\": initial_balance,\n","          \"window_size\": window_size,\n","          \"transaction_cost_pct\": transaction_cost_pct,\n","          \"sharpe_window\": sharpe_window,\n","          \"sharpe_ma_window\": sharpe_ma_window\n","      },\n","  )\n","  return run\n","\n","run = wandb_init()"]},{"cell_type":"markdown","id":"ckJEJgccoU8H","metadata":{"id":"ckJEJgccoU8H"},"source":["Hier ist der Weights and Biases Code zum tracken der Konfigurationen der Experimente."]},{"cell_type":"markdown","id":"1bf3db48","metadata":{"id":"1bf3db48"},"source":["# Environment Class"]},{"cell_type":"code","execution_count":null,"id":"d66b3a87","metadata":{"id":"d66b3a87"},"outputs":[],"source":["# (ZELLE 7 KORRIGIERT)\n","class PortfolioEnv(gym.Env):\n","    \"\"\"\n","    Eine Gymnasium-Umgebung fÃ¼r das Portfolio-Management, die Daten aus einer\n","    CSV-Datei lÃ¤dt und eine Trailing-Sharpe-Ratio als Belohnung verwendet.\n","    \"\"\"\n","\n","    metadata = {\"render_modes\": [\"human\"]}\n","\n","    def __init__(\n","        self,\n","        csv_path: str,\n","        return_cols: List[str],\n","        initial_balance: float = initial_balance,\n","        window_size: int = window_size,\n","        transaction_cost_pct: float = transaction_cost_pct,\n","        sharpe_window: int = sharpe_window,\n","    ):\n","\n","        super().__init__()\n","\n","        # --- Parameter speichern ---\n","        self.csv_path = csv_path\n","        self.window_size = window_size\n","        self.initial_balance = initial_balance\n","        self.transaction_cost_pct = transaction_cost_pct\n","        self.sharpe_window = sharpe_window\n","\n","        # --- Asset-Konfiguration ---\n","        self.asset_return_columns = return_cols\n","        self.num_assets = len(self.asset_return_columns)\n","        self.num_portfolio_components = self.num_assets + 1\n","\n","        # --- Daten laden ---\n","        self._load_data_csv() # Diese Funktion ist jetzt korrigiert\n","\n","        # --- Interne Zustandsvariablen ---\n","        self.start_tick = self.window_size\n","        self.end_tick = len(self.df) - 1\n","\n","        self.current_step = 0\n","        self.portfolio_value = 0.0\n","        self.prev_portfolio_value = 0.0\n","        self.portfolio_weights = np.zeros(\n","            self.num_portfolio_components, dtype=np.float32\n","        )\n","        self.done = False\n","        self._return_history = []\n","        self.sharpe_ma_window = sharpe_ma_window # Hyperparameter: GlÃ¤ttet die Sharpe Ã¼ber 10 Tage\n","        self._sharpe_history = []\n","\n","        # --- 1. Action Space (Kontinuierlich) ---\n","        self.action_space = spaces.Box(\n","            low=0.0, high=1.0, shape=(self.num_portfolio_components,), dtype=np.float32\n","        )\n","\n","        # --- 2. Observation Space (Dictionary) ---\n","        self.observation_space = spaces.Dict(\n","            {\n","                \"market_data\": spaces.Box(\n","                    low=-np.inf,\n","                    high=np.inf,\n","                    shape=(self.window_size, self.num_assets),\n","                    dtype=np.float32,\n","                ),\n","                \"portfolio_weights\": spaces.Box(\n","                    low=0.0,\n","                    high=1.0,\n","                    shape=(self.num_portfolio_components,),\n","                    dtype=np.float32,\n","                ),\n","            }\n","        )\n","\n","    def _load_data_csv(self):\n","        try:\n","            # --- KORREKTUR HIER ---\n","            # self.csv_path (die Klassenvariable) verwenden, nicht die globale 'csv_path'\n","            self.df = pd.read_csv(self.csv_path)\n","            # --------------------\n","\n","            if \"Date\" in self.df.columns:\n","                self.df[\"Date\"] = pd.to_datetime(self.df[\"Date\"])\n","                self.df.set_index(\"Date\", inplace=True)\n","\n","            if len(self.df) < self.window_size + 1:\n","                raise ValueError(\n","                    f\"Nicht genÃ¼gend Daten ({len(self.df)}) fÃ¼r window_size={self.window_size}.\"\n","                )\n","            print(\n","                f\"Daten erfolgreich geladen: {len(self.df)} Zeitschritte, {self.num_assets} Assets.\"\n","            )\n","        except FileNotFoundError:\n","            print(f\"Fehler: CSV-Datei {self.csv_path} nicht gefunden.\")\n","            raise\n","\n","    # (Rest der PortfolioEnv-Klasse bleibt unverÃ¤ndert)\n","    # ... (def _get_observation, def _calculate_reward, def reset, def step) ...\n","    # (Du musst den Rest der Klasse von deinem Original-Code kopieren, da er hier abgeschnitten ist)\n","    def _get_observation(self) -> Dict[str, np.ndarray]:\n","        end_idx = self.current_step\n","        start_idx = end_idx - self.window_size\n","        market_data = self.df[self.asset_return_columns].iloc[start_idx:end_idx].values\n","\n","        obs_dict = {\n","            \"market_data\": market_data.astype(np.float32),\n","            \"portfolio_weights\": self.portfolio_weights.astype(np.float32),\n","        }\n","        return obs_dict\n","\n","    def _calculate_reward(self) -> float:\n","        if self.prev_portfolio_value <= 1e-6:\n","            return 0.0\n","        reward = np.log(self.portfolio_value / self.prev_portfolio_value)\n","        if np.isnan(reward) or not np.isfinite(reward):\n","            reward = -10.0\n","        return reward\n","\n","    def _calculate_reward_sharpe(self) -> float:\n","        if self.prev_portfolio_value > 1e-6:\n","            step_return = (self.portfolio_value / self.prev_portfolio_value) - 1.0\n","        else:\n","            step_return = 0.0\n","        self._return_history.append(step_return)\n","        current_sharpe = 0.0\n","        if len(self._return_history) >= self.sharpe_window:\n","            recent_returns = np.array(self._return_history[-self.sharpe_window:])\n","            mean_return = np.mean(recent_returns)\n","            std_dev = np.std(recent_returns)\n","            if std_dev > 1e-6:\n","                current_sharpe = (mean_return / std_dev) * np.sqrt(252)\n","        self._sharpe_history.append(current_sharpe)\n","        if len(self._sharpe_history) < self.sharpe_ma_window:\n","            return 0.0\n","        ma_sharpe = np.mean(self._sharpe_history[-self.sharpe_ma_window:])\n","        reward = np.clip(ma_sharpe, -5.0, 5.0)\n","        return reward\n","\n","    def reset(\n","        self, seed: Optional[int] = None, options: Optional[dict] = None\n","    ) -> (dict, dict):\n","        super().reset(seed=seed)\n","        self.current_step = self.start_tick\n","        self.portfolio_value = self.initial_balance\n","        self.prev_portfolio_value = self.initial_balance\n","        self.portfolio_weights = np.zeros(\n","            self.num_portfolio_components, dtype=np.float32\n","        )\n","        self.portfolio_weights[0] = 1.0\n","        self.done = False\n","        self._return_history = []\n","        observation = self._get_observation()\n","        info = {}\n","        self._sharpe_history = []\n","        return observation, info\n","\n","    def step(self, action: np.ndarray) -> (dict, float, bool, bool, dict):\n","        if self.done:\n","            return (\n","                self._get_observation(),\n","                0.0,\n","                True,\n","                False,\n","                {\"info\": \"Episode already finished.\"},\n","            )\n","        self.prev_portfolio_value = self.portfolio_value\n","        old_actual_weights = self.portfolio_weights\n","        action = np.maximum(action, 0.0)\n","        action_sum = np.sum(action)\n","        if action_sum > 1e-6:\n","             target_weights = action / action_sum\n","        else:\n","             target_weights = np.zeros(self.num_portfolio_components, dtype=np.float32)\n","             target_weights[0] = 1.0\n","        turnover = np.sum(np.abs(target_weights[1:] - old_actual_weights[1:]))\n","        costs = turnover * self.prev_portfolio_value * self.transaction_cost_pct\n","        portfolio_value_after_costs = self.prev_portfolio_value - costs\n","        self.portfolio_weights = target_weights\n","        current_returns = (\n","            self.df[self.asset_return_columns].iloc[self.current_step].values\n","        )\n","        returns_vector = np.concatenate(([0.0], current_returns)).astype(np.float32)\n","        self.portfolio_value = portfolio_value_after_costs * np.sum(self.portfolio_weights * (1 + returns_vector))\n","        if self.portfolio_value <= 1e-6:\n","            self.portfolio_value = 0.0\n","            self.done = True\n","        reward = self._calculate_reward()\n","        self.current_step += 1\n","        if self.current_step > self.end_tick:\n","            self.done = True\n","        observation = self._get_observation()\n","        info = {\"portfolio_value\": self.portfolio_value, \"transaction_costs\": costs}\n","        terminated = self.done\n","        truncated = False\n","        return observation, reward, terminated, truncated, info\n","\n","    def close(self):\n","        pass"]},{"cell_type":"markdown","id":"5c1136da","metadata":{"id":"5c1136da"},"source":["# 1. Das Keras-Modell (Das \"Gehirn\" ğŸ§ )"]},{"cell_type":"code","execution_count":null,"id":"0cfeb446","metadata":{"id":"0cfeb446"},"outputs":[],"source":["# Wir importieren die Verteilungs-Tools von TFP\n","tfd = tfp.distributions\n","\n","def create_actor_critic_model(env: gym.Env) -> (tf.keras.Model, Callable):\n","    \"\"\"\n","    Erstellt ein Keras Actor-Critic-Modell (1 Stamm, 2 KÃ¶pfe),\n","    das dynamisch auf den 'spaces.Dict' Observation Space der Umgebung passt\n","    und die Parameter fÃ¼r eine Dirichlet-Verteilung ausgibt.\n","\n","    Args:\n","        env (gym.Env): Eine Instanz deiner PortfolioEnv, die\n","                       env.observation_space[\"market_data\"] und\n","                       env.observation_space[\"portfolio_weights\"] bereitstellt.\n","\n","    Returns:\n","        (tf.keras.Model): Das kompilierbare Keras-Modell.\n","        (Callable): Eine Funktion, die Alpha-Parameter entgegennimmt und\n","                    eine Dirichlet-Verteilung zurÃ¼ckgibt.\n","    \"\"\"\n","\n","    # --- 1. Definiere die Inputs (dynamisch aus der Env) ---\n","    market_input = Input(\n","        shape=env.observation_space[\"market_data\"].shape,\n","        name=\"market_data\",  # <-- Muss \"market_data\" heiÃŸen\n","    )\n","\n","    weights_input = Input(\n","        shape=env.observation_space[\"portfolio_weights\"].shape,\n","        name=\"portfolio_weights\",  # <-- Muss \"portfolio_weights\" heiÃŸen\n","    )\n","\n","    # --- 2. Gemeinsamer Stamm (Body) ---\n","\n","    # Das LSTM verarbeitet die 2D-Marktdaten\n","    # (64 Neuronen ist ein Hyperparameter zum Tunen)\n","    lstm_out = LSTM(64, name=\"lstm_stamm\")(market_input)\n","\n","    # Kombiniere den LSTM-Output (Markt-Kontext) mit den\n","    # aktuellen Gewichten (aktueller Zustand)\n","    concatenated_features = Concatenate(name=\"kombinierte_features\")(\n","        [lstm_out, weights_input]\n","    )\n","\n","    # Ein gemeinsamer Dense-Layer fÃ¼r die kombinierten Features\n","    # (32 Neuronen ist ein Hyperparameter)\n","    shared_dense = Dense(32, activation=\"relu\", name=\"gemeinsamer_dense\")(\n","        concatenated_features\n","    )\n","\n","    # --- 3. Definiere die 2 \"KÃ¶pfe\" ---\n","\n","    # A) Der Actor-Kopf (gibt ALPHAS fÃ¼r die Dirichlet-Verteilung aus)\n","    actor_output_size = env.action_space.shape[0]  # (Assets + Cash)\n","\n","    # Wir brauchen positive Alphas.\n","    # 'softplus' + 1.0 ist ein gÃ¤ngiger Trick, um alpha > 1 zu garantieren,\n","    # was die Verteilung stabilisiert (vermeidet extreme 0/1-Gewichte).\n","    alpha_params = Dense(\n","        actor_output_size,\n","        activation=lambda x: tf.nn.softplus(x) + 1.0,\n","        name=\"alpha_params\",\n","    )(shared_dense)\n","\n","    # B) Der Critic-Kopf (gibt den Wert des Zustands aus)\n","    # Nur ein Neuron, 'linear' (keine Aktivierung), da es einen Wert schÃ¤tzt.\n","    critic_head = Dense(1, activation=\"linear\", name=\"critic_output\")(shared_dense)\n","\n","    # --- 4. Erstelle das finale Modell ---\n","    # Das Modell weiÃŸ jetzt, dass es ein Dictionary mit den SchlÃ¼sseln\n","    # \"market_data\" und \"portfolio_weights\" als Input erwartet.\n","    model = Model(\n","        inputs=[market_input, weights_input],\n","        outputs=[alpha_params, critic_head],\n","        name=\"Actor_Critic_Dirichlet_Modell\",\n","    )\n","\n","    # --- 5. Hilfsfunktion fÃ¼r die Verteilung ---\n","    # Diese Funktion wird im Trainings-Loop verwendet, um aus den\n","    # Alpha-Parametern eine Verteilung zu erstellen.\n","    def create_distribution(alpha_params: tf.Tensor) -> tfd.Dirichlet:\n","        # Erstellt eine Dirichlet-Verteilung aus den Alpha-Parametern\n","        return tfd.Dirichlet(alpha_params)\n","\n","    return model, create_distribution"]},{"cell_type":"markdown","id":"84c51ab0","metadata":{"id":"84c51ab0"},"source":["# 2. Der Trainings-Schritt (Das \"Nervensystem\" mit GradientTape)"]},{"cell_type":"code","execution_count":null,"id":"31401d06","metadata":{"id":"31401d06"},"outputs":[],"source":["def calculate_gae(rewards, values, next_values, dones, gamma=0.99, lambda_gae=0.95):\n","    \"\"\"\n","    Berechnet die Generalized Advantage Estimation (GAE).\n","    \"\"\"\n","    advantages = np.zeros_like(rewards)\n","    last_gae_lam = 0\n","\n","    # (1.0 - dones) stellt sicher, dass der Wert 0 ist, wenn die Episode endet\n","    non_terminal = 1.0 - dones\n","\n","    for t in reversed(range(len(rewards))):\n","        # Berechne den TD-Error (die \"Ãœberraschung\")\n","        delta = rewards[t] + gamma * next_values[t] * non_terminal[t] - values[t]\n","\n","        # Berechne den GAE-Advantage\n","        last_gae_lam = delta + gamma * lambda_gae * non_terminal[t] * last_gae_lam\n","        advantages[t] = last_gae_lam\n","\n","    # 'targets' sind die Advantages + die WertschÃ¤tzungen\n","    targets = advantages + values\n","    return advantages, targets\n","\n","@tf.function\n","def train_step(\n","    model, create_dist, observations_dict, actions, old_log_probs, advantages, targets\n","):\n","    \"\"\"\n","    FÃ¼hrt den PPO-Trainingsschritt mit tf.GradientTape aus.\n","    \"\"\"\n","    with tf.GradientTape() as tape:\n","        # --- 1. Forward Pass (Neue Werte berechnen) ---\n","        # Hole die *neuen* Alpha-Parameter und den *neuen* Wert\n","        new_alphas, new_critic_value = model(observations_dict, training=True)\n","\n","        # Erstelle die *neue* Verteilung\n","        new_dist = create_dist(new_alphas)\n","\n","        # Berechne die Log-Wahrscheinlichkeit der *alten* Aktionen unter der *neuen* Policy\n","        new_log_probs = new_dist.log_prob(actions)\n","\n","        # --- 2. Critic-Verlust (Value Loss) ---\n","        # Rufe die Instanz auf, die wir oben erstellt haben\n","        critic_loss = critic_loss_fn(targets, new_critic_value)  # <-- KORRIGIERTE ZEILE\n","\n","        # tf.reduce_mean ist nicht mehr nÃ¶tig, da die Klasse das automatisch macht\n","        critic_loss = critic_loss * vf_loss_coeff\n","\n","        # --- 3. Actor-Verlust (PPO-Loss) ---\n","        # Berechne das Wahrscheinlichkeits-VerhÃ¤ltnis (Ratio)\n","        ratio = tf.exp(new_log_probs - old_log_probs)\n","\n","        # Clipped-Verlust\n","        clipped_ratio = tf.clip_by_value(\n","            ratio, 1 - ppo_clip_epsilon, 1 + ppo_clip_epsilon\n","        )\n","\n","        actor_loss_unclipped = ratio * advantages\n","        actor_loss_clipped = clipped_ratio * advantages\n","\n","        # Nimm das Minimum (schlechteste von beiden), um \"gierige\" Updates zu bestrafen\n","        actor_loss = -tf.reduce_mean(\n","            tf.minimum(actor_loss_unclipped, actor_loss_clipped)\n","        )\n","\n","        # --- 4. Entropie-Verlust (fÃ¼r Exploration) ---\n","        # Wir wollen die Entropie MAXIMIEREN, also MINIMIEREN wir die negative Entropie\n","        entropy_loss = -tf.reduce_mean(new_dist.entropy())\n","\n","        # --- 5. Gesamtverlust ---\n","        total_loss = actor_loss + critic_loss + (entropy_loss * entropy_coeff)\n","\n","    # --- 6. Gradienten berechnen & anwenden ---\n","    gradients = tape.gradient(total_loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","    return total_loss, actor_loss, critic_loss, entropy_loss"]},{"cell_type":"code","execution_count":null,"id":"ace11a70","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":744},"id":"ace11a70","outputId":"1feb0ef9-a4bb-44f3-fc36-7f27897a53ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Initialisiere Umgebung und Modell fÃ¼r DS1_Winner_BH.csv ---\n","Gefundene Return-Spalten: ['Winner_Return', 'Loser_Return']\n","Daten erfolgreich geladen: 200 Zeitschritte, 2 Assets.\n"]},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Actor_Critic_Dirichlet_Modell\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"Actor_Critic_Dirichlet_Modell\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n","â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚ market_data         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)     â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n","â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ lstm_stamm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">17,152</span> â”‚ market_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ portfolio_weights   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n","â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ kombinierte_featurâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">67</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ lstm_stamm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], â”‚\n","â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       â”‚                   â”‚            â”‚ portfolio_weightâ€¦ â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ gemeinsamer_dense   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,176</span> â”‚ kombinierte_featâ€¦ â”‚\n","â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚                   â”‚            â”‚                   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ alpha_params        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> â”‚ gemeinsamer_densâ€¦ â”‚\n","â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚                   â”‚            â”‚                   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ critic_output       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> â”‚ gemeinsamer_densâ€¦ â”‚\n","â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             â”‚                   â”‚            â”‚                   â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","</pre>\n"],"text/plain":["â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n","â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n","â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n","â”‚ market_data         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m2\u001b[0m)     â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n","â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ lstm_stamm (\u001b[38;5;33mLSTM\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚     \u001b[38;5;34m17,152\u001b[0m â”‚ market_data[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ portfolio_weights   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n","â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ kombinierte_featurâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m67\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ lstm_stamm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], â”‚\n","â”‚ (\u001b[38;5;33mConcatenate\u001b[0m)       â”‚                   â”‚            â”‚ portfolio_weightâ€¦ â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ gemeinsamer_dense   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        â”‚      \u001b[38;5;34m2,176\u001b[0m â”‚ kombinierte_featâ€¦ â”‚\n","â”‚ (\u001b[38;5;33mDense\u001b[0m)             â”‚                   â”‚            â”‚                   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ alpha_params        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         â”‚         \u001b[38;5;34m99\u001b[0m â”‚ gemeinsamer_densâ€¦ â”‚\n","â”‚ (\u001b[38;5;33mDense\u001b[0m)             â”‚                   â”‚            â”‚                   â”‚\n","â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n","â”‚ critic_output       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         â”‚         \u001b[38;5;34m33\u001b[0m â”‚ gemeinsamer_densâ€¦ â”‚\n","â”‚ (\u001b[38;5;33mDense\u001b[0m)             â”‚                   â”‚            â”‚                   â”‚\n","â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,460</span> (76.02 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,460\u001b[0m (76.02 KB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,460</span> (76.02 KB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,460\u001b[0m (76.02 KB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Finishing previous runs because reinit is set to True."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">ruby-wave-75</strong> at: <a href='https://wandb.ai/cb-ml/deep_learning_reinforcement_learning/runs/4znxng5a' target=\"_blank\">https://wandb.ai/cb-ml/deep_learning_reinforcement_learning/runs/4znxng5a</a><br> View project at: <a href='https://wandb.ai/cb-ml/deep_learning_reinforcement_learning' target=\"_blank\">https://wandb.ai/cb-ml/deep_learning_reinforcement_learning</a><br>Synced 5 W&B file(s), 0 media file(s), 11 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20251105_190030-4znxng5a/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.22.2"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20251105_190056-eup79yys</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/cb-ml/PPO_Portfolio_Sweep/runs/eup79yys' target=\"_blank\">DS1_Winner_BH.csv (lr=0.001)</a></strong> to <a href='https://wandb.ai/cb-ml/PPO_Portfolio_Sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/cb-ml/PPO_Portfolio_Sweep' target=\"_blank\">https://wandb.ai/cb-ml/PPO_Portfolio_Sweep</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/cb-ml/PPO_Portfolio_Sweep/runs/eup79yys' target=\"_blank\">https://wandb.ai/cb-ml/PPO_Portfolio_Sweep/runs/eup79yys</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Starte Training...\n","Iter 0: Reward=0.0000, Critic Loss=3.6880\n","Iter 20: Reward=0.0008, Critic Loss=4.1916\n"]}],"source":["# --- Datensatz-Pfade (global definieren) ---\n","BASE_DIR = \"/content/drive/MyDrive/Data/PPO_portfolio_optimization/\"\n","scenario_keys = [\n","    \"DS1_Winner_BH\", \"DS2_Perfect_Timing\", \"DS3_Signal_in_Noise\",\n","    \"DS4_Risk_Choice\", \"DS5_Regime_Switch\", \"DS6_Correlation_Trap\",\n","    \"DS7_Needle_in_Haystack\", \"DS8_Sector_Diversification\",\n","    \"DS9_Factor_Timing\", \"DS10_SP100_Sim\"\n","]\n","dataset_paths = [os.path.join(BASE_DIR, f\"{key}.csv\") for key in scenario_keys]\n","\n","# --- W&B Login (global, einmalig) ---\n","# (Stelle sicher, dass wandb.login() in einer vorherigen Zelle aufgerufen wurde)\n","\n","\n","# --- 3. Starte den \"Sweep\" Ã¼ber alle DatensÃ¤tze ---\n","for path in dataset_paths:\n","\n","    # --- A. Initialisierung (WIRD JETZT IN JEDER SCHLEIFE AUSGEFÃœHRT) ---\n","    print(f\"\\n--- Initialisiere Umgebung und Modell fÃ¼r {os.path.basename(path)} ---\")\n","\n","    CSV_FILE_PATH = path\n","\n","    # --- KORREKTUR FÃœR FEHLER 1 (KeyError) ---\n","    try:\n","        all_cols = pd.read_csv(CSV_FILE_PATH, nrows=0).columns.tolist()\n","        RETURN_COLUMNS = [col for col in all_cols if col != 'Date']\n","        print(f\"Gefundene Return-Spalten: {RETURN_COLUMNS}\")\n","\n","        # PrÃ¼fe, ob Ã¼berhaupt Spalten gefunden wurden (wichtig fÃ¼r DS8-Fehler)\n","        if not RETURN_COLUMNS:\n","            print(f\"WARNUNG: Keine Return-Spalten in {path} gefunden. Ãœberspringe.\")\n","            continue # Gehe zur nÃ¤chsten Datei\n","    except Exception as e:\n","        print(f\"Konnte CSV nicht lesen oder Spalten nicht finden: {e}\")\n","        continue\n","    # -------------------------------------------\n","\n","    env = PortfolioEnv(\n","        csv_path=CSV_FILE_PATH,\n","        return_cols=RETURN_COLUMNS,\n","        window_size=window_size,\n","        sharpe_window=sharpe_window\n","    )\n","\n","    model, create_distribution = create_actor_critic_model(env)\n","\n","    # --- KORREKTUR FÃœR FEHLER 2 (NotImplementedError) ---\n","    # Definiere Optimizer und Loss-Funktion HIER, INNENHALB der Schleife\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    critic_loss_fn = tf.keras.losses.MeanSquaredError()\n","    # --------------------------------------------------\n","\n","    # --- KORREKTUR FÃœR FEHLER 2 (NotImplementedError) ---\n","    # Definiere die kompilierte Funktion HIER, INNENHALB der Schleife\n","    @tf.function\n","    def train_step(model, create_dist, optimizer, observations_dict, actions, old_log_probs, advantages, targets):\n","        \"\"\"\n","        FÃ¼hrt den PPO-Trainingsschritt mit tf.GradientTape aus.\n","        \"\"\"\n","        with tf.GradientTape() as tape:\n","            new_alphas, new_critic_value = model(observations_dict, training=True)\n","            new_dist = create_dist(new_alphas)\n","\n","            actions_clipped = tf.clip_by_value(actions, 1e-10, 1.0)\n","            new_log_probs = new_dist.log_prob(actions_clipped)\n","\n","            critic_loss = critic_loss_fn(targets, new_critic_value)\n","            critic_loss = critic_loss * vf_loss_coeff\n","\n","            ratio = tf.exp(new_log_probs - old_log_probs)\n","            clipped_ratio = tf.clip_by_value(ratio, 1 - ppo_clip_epsilon, 1 + ppo_clip_epsilon)\n","            actor_loss_unclipped = ratio * advantages\n","            actor_loss_clipped = clipped_ratio * advantages\n","            actor_loss = -tf.reduce_mean(tf.minimum(actor_loss_unclipped, actor_loss_clipped))\n","\n","            entropy_loss = -tf.reduce_mean(new_dist.entropy())\n","\n","            total_loss = actor_loss + critic_loss + (entropy_loss * entropy_coeff)\n","\n","        gradients = tape.gradient(total_loss, model.trainable_variables)\n","        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","        return total_loss, actor_loss, critic_loss, entropy_loss\n","    # ----------------------------------------------------\n","\n","    model.summary()\n","\n","    # --- B. Starte W&B Run fÃ¼r dieses Szenario ---\n","    run = wandb.init(\n","        project=\"PPO_Portfolio_Sweep\",\n","        name=f\"{os.path.basename(path)} (lr={learning_rate})\",\n","        config={\n","            \"scenario\": os.path.basename(path),\n","            \"learning_rate\": learning_rate,\n","            \"window_size\": window_size,\n","            \"num_iterations\": num_iterations,\n","            # (FÃ¼ge hier alle anderen globalen Hyperparameter hinzu)\n","        },\n","        reinit=True\n","    )\n","    print(\"Starte Training...\")\n","\n","    # --- C. Haupt-Trainings-Loop (UnverÃ¤ndert) ---\n","    for iteration in range(num_iterations):\n","\n","        # --- PHASE 1: DATENSAMMELN ---\n","        batch_obs, batch_actions, batch_rewards = [], [], []\n","        batch_old_log_probs, batch_values, batch_dones, batch_next_obs = [], [], [], []\n","\n","        obs, info = env.reset()\n","\n","        for t in range(steps_per_rollout):\n","            obs_market = np.expand_dims(obs[\"market_data\"], axis=0)\n","            obs_weights = np.expand_dims(obs[\"portfolio_weights\"], axis=0)\n","            model_inputs = {\"market_data\": obs_market, \"portfolio_weights\": obs_weights}\n","\n","            alphas, critic_value = model(model_inputs)\n","            dist = create_distribution(alphas)\n","            action = dist.sample()[0].numpy()\n","            action = np.clip(action, 1e-10, 1.0)\n","            action = action / np.sum(action)\n","            old_log_prob = dist.log_prob(action).numpy()\n","\n","            next_obs, reward, terminated, truncated, info = env.step(action)\n","            done = terminated or truncated\n","\n","            batch_obs.append(obs); batch_actions.append(action); batch_rewards.append(reward)\n","            batch_old_log_probs.append(old_log_prob[0]); batch_values.append(critic_value.numpy()[0][0])\n","            batch_dones.append(done); batch_next_obs.append(next_obs)\n","\n","            obs = next_obs\n","            if done:\n","                obs, info = env.reset()\n","\n","        # --- PHASE 2: ADVANTAGES BERECHNEN ---\n","        obs_market_next = np.array([o[\"market_data\"] for o in batch_next_obs])\n","        obs_weights_next = np.array([o[\"portfolio_weights\"] for o in batch_next_obs])\n","        last_value_inputs = {\n","            \"market_data\": obs_market_next[-1:],\n","            \"portfolio_weights\": obs_weights_next[-1:]\n","        }\n","\n","        # Behandele den Fall, dass der Batch leer ist (falls steps_per_rollout < 1)\n","        if not batch_values:\n","            print(f\"WARNUNG: Kein einziger Schritt in Iteration {iteration} ausgefÃ¼hrt. Ãœberspringe Training.\")\n","            continue\n","\n","        last_value = model(last_value_inputs)[1].numpy()[0][0]\n","\n","        all_values = np.array(batch_values + [last_value], dtype=np.float32)\n","        batch_rewards_np = np.array(batch_rewards, dtype=np.float32)\n","        batch_dones_np = np.array(batch_dones, dtype=np.float32)\n","\n","        rewards_mean = np.mean(batch_rewards_np); rewards_std = np.std(batch_rewards_np) + 1e-8\n","        normalized_rewards = (batch_rewards_np - rewards_mean) / rewards_std\n","\n","        advantages, targets = calculate_gae(\n","            rewards=normalized_rewards, values=all_values[:-1], next_values=all_values[1:],\n","            dones=batch_dones_np, gamma=gamma, lambda_gae=lambda_gae\n","        )\n","\n","        advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)\n","\n","        # --- PHASE 3: TRAINIEREN ---\n","        obs_market_tensor = tf.convert_to_tensor([o[\"market_data\"] for o in batch_obs])\n","        obs_weights_tensor = tf.convert_to_tensor([o[\"portfolio_weights\"] for o in batch_obs])\n","        obs_dict_tensor = {\"market_data\": obs_market_tensor, \"portfolio_weights\": obs_weights_tensor}\n","\n","        actions_tensor = tf.convert_to_tensor(batch_actions, dtype=tf.float32)\n","        old_log_probs_tensor = tf.convert_to_tensor(batch_old_log_probs, dtype=tf.float32)\n","        advantages_tensor = tf.convert_to_tensor(advantages, dtype=tf.float32)\n","        targets_tensor = tf.convert_to_tensor(targets, dtype=tf.float32)\n","\n","        total_size = len(batch_obs)\n","        indices = np.arange(total_size)\n","        epoch_total_loss, epoch_actor_loss, epoch_critic_loss, epoch_entropy_loss = [], [], [], []\n","\n","        for _ in range(num_ppo_epochs):\n","            np.random.shuffle(indices)\n","            for start in range(0, total_size, mini_batch_size):\n","                end = start + mini_batch_size\n","                mb_indices = indices[start:end]\n","\n","                mb_obs_dict = {\n","                    \"market_data\": tf.gather(obs_market_tensor, mb_indices),\n","                    \"portfolio_weights\": tf.gather(obs_weights_tensor, mb_indices)\n","                }\n","                mb_actions = tf.gather(actions_tensor, mb_indices)\n","                mb_old_log_probs = tf.gather(old_log_probs_tensor, mb_indices)\n","                mb_advantages = tf.gather(advantages_tensor, mb_indices)\n","                mb_targets = tf.gather(targets_tensor, mb_indices)\n","\n","                # Rufe train_step auf\n","                t_loss, a_loss, c_loss, e_loss = train_step(\n","                    model, create_distribution, optimizer,\n","                    mb_obs_dict, mb_actions, mb_old_log_probs, mb_advantages, mb_targets\n","                )\n","\n","                epoch_total_loss.append(t_loss.numpy()); epoch_actor_loss.append(a_loss.numpy())\n","                epoch_critic_loss.append(c_loss.numpy()); epoch_entropy_loss.append(e_loss.numpy())\n","\n","        # --- PHASE 4: Logging ---\n","        mean_reward = np.mean(batch_rewards)\n","        wandb.log({\n","            \"Mean Reward\": mean_reward,\n","            \"Mean Total Loss\": np.mean(epoch_total_loss),\n","            \"Mean Actor Loss\": np.mean(epoch_actor_loss),\n","            \"Mean Critic Loss\": np.mean(epoch_critic_loss),\n","            \"Entropy\": np.mean(epoch_entropy_loss)\n","        }, step=iteration)\n","\n","        if iteration % 20 == 0:\n","            print(f\"Iter {iteration}: Reward={mean_reward:.4f}, Critic Loss={np.mean(epoch_critic_loss):.4f}\")\n","\n","    wandb.finish()\n","    print(f\"Training fÃ¼r {path} abgeschlossen.\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1i4wfs3-Dob62BPHeANhv0q1KGNtv8D6r","timestamp":1762413281132}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.19"}},"nbformat":4,"nbformat_minor":5}